{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis: Bankruptcy Prediction for European Countries\n",
    "Code written by Marc Zeugin (UZH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, PrecisionRecallDisplay, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearnex.ensemble import RandomForestClassifier\n",
    "from sklearnex.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import IterativeImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from skopt.space import Real, Integer\n",
    "from matplotlib import pyplot as plt\n",
    "from skopt import BayesSearchCV\n",
    "from joblib import dump, Memory\n",
    "from sklearnex.svm import SVC\n",
    "from itertools import product\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "import pyarrow\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine training/testing ratio, default is 0.2\n",
    "tt_size = 0.2\n",
    "# number of splits for k-fold crossvalidation, default is 5\n",
    "k_splits = 5\n",
    "# set number of jobs to run in parallel, -1 means all processors, default is 1\n",
    "jobs = -1\n",
    "# number of iterations for BayesSearchCV\n",
    "n_iterations = 40\n",
    "# select scoring model to optimize, default is average_precision, the value used for the precision-recall curve\n",
    "scoring_metric = 'average_precision'\n",
    "# label for classification report\n",
    "label = ['Non-Bankrupt', 'Bankrupt']\n",
    "\n",
    "# absolute path to dataset\n",
    "path = 'C:/Users/marczeugin/Documents/Masterthesis/datasets/'\n",
    "# extension of dataset type\n",
    "ext = '*.csv'\n",
    "\n",
    "# determine the size (in inches) of the precision-recall curve figure, default is (4, 2)\n",
    "figure_size = (4, 2)\n",
    "# set dpi of small graphs, default is 100\n",
    "dpi_low = 100\n",
    "# set dpi of medium graphs, default is 200\n",
    "dpi_med = 200\n",
    "# set dpi of large graphs, default is 250\n",
    "dpi_high = 250\n",
    "\n",
    "# enable subsample of total dataset to be used for hyperparameter tuning, default is True\n",
    "allow_subsample = True\n",
    "# set subsample size of total dataset, default is .1 e.g. 10%\n",
    "subsample_size = 0.1\n",
    "# enable quick overview to run with subsample\n",
    "allow_subsample_overview = True\n",
    "# enable hyperparameter tuning with subsample\n",
    "allow_subsample_hyperparameter = True\n",
    "\n",
    "# allow to load dataset with SMOTEENN already run and imputation already imputed, default is True\n",
    "allow_computed_set = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for file_name in glob.glob(path+ext):\n",
    "    df = pd.read_csv(file_name, na_values=['n.a.',0], index_col=False)\n",
    "    df.drop(columns=df.columns[-2:], axis=1, inplace=True)\n",
    "    df.drop(columns=['Company name Latin alphabet', 'Quoted', 'Branch', 'OwnData', 'Woco', 'NACE Rev. 2 core code (4 digits)'], axis=1, inplace=True)\n",
    "    li.append(df)\n",
    "\n",
    "bankruptcy_comp_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "print(bankruptcy_comp_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initial dataset manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Set bankrupt to 1 or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_comp_df = bankruptcy_comp_df.replace({'Inactive': {\"Yes\": 1, \"No\": 0}})\n",
    "bankruptcy_comp_df['Inactive'] = bankruptcy_comp_df['Inactive'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Rename bankrupt column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_comp_df.rename(columns={'Inactive': 'Bankrupt', 'GNI growth last Year': 'GNI last Year', 'GNI growth Year - 1': 'GNI Year - 1', 'GNI growth Year - 2': 'GNI Year - 2', \n",
    "                                   'GNI growth Year - 3': 'GNI Year - 3', 'GNI growth Year - 4': 'GNI Year - 4', 'GNI growth Year - 5': 'GNI Year - 5'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Remove from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Remove data based on year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rows = bankruptcy_comp_df.shape[0]\n",
    "bankruptcy_comp_df = bankruptcy_comp_df[bankruptcy_comp_df['Last avail. Year'] > 2000]\n",
    "bankruptcy_comp_df = bankruptcy_comp_df[bankruptcy_comp_df['Last avail. Year'] < 2021]\n",
    "new_rows = bankruptcy_comp_df.shape[0]\n",
    "print(f'Removed a total of {original_rows - new_rows} rows, that is {round(100-new_rows/original_rows*100,4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Remove data based on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_comp_df = bankruptcy_comp_df[bankruptcy_comp_df.isnull().sum(axis=1) <= bankruptcy_comp_df.shape[1]/4]\n",
    "new_rows = bankruptcy_comp_df.shape[0]\n",
    "print(f'Removed a total of {original_rows - new_rows} rows, that is {round(100-new_rows/original_rows*100,4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Display basic information of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Initial dataset shape: {bankruptcy_comp_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Check dataset quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = bankruptcy_comp_df.isna().sum().sum()\n",
    "print(f'Total missing values: {missing_values_count}')\n",
    "print('--'*60)\n",
    "duplicates = bankruptcy_comp_df.duplicated().sum()\n",
    "print('Any duplicated values: ' + str(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if duplicates > 0:\n",
    "    bankruptcy_comp_df.drop_duplicates(inplace=True)\n",
    "    print(f'Dataset shape after removing duplicates: {bankruptcy_comp_df.shape}')\n",
    "    duplicates = bankruptcy_comp_df.duplicated().sum()\n",
    "    print('Duplicated values left: ' + str(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Check missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bankruptcy_comp_df.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3. Remove columns with more than 50% missing values\n",
    "This removes 5 columns with more than 99.89% missing values identified in 2.4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_comp_df.drop(columns=['EBIT Year - 4', 'EBIT Year - 3', 'EBIT Year - 2', 'EBIT Year - 1', 'EBIT last Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = bankruptcy_comp_df.isna().sum().sum()\n",
    "print(f'Total missing values: {missing_values_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4. Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bankruptcy_comp_df.drop(columns=['Bankrupt'], axis=1)\n",
    "y = bankruptcy_comp_df['Bankrupt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_subsample:\n",
    "    subsample_size_inversed = (1 - tt_size)/(1 - subsample_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tt_size, random_state=1, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5. Impute train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_computed_set:\n",
    "    X_train = pd.read_feather(path+'0after_imputation_train.feather')\n",
    "    X_train = X_train.set_index('index')\n",
    "elif missing_values_count > 0:\n",
    "    X_train = X_train.replace('NaN', np.nan)\n",
    "    values = X_train.iloc[:,2:].values\n",
    "    imputer = IterativeImputer(random_state=1, tol=1e-8)\n",
    "    imputer.fit(values)\n",
    "    imputed_train_values = imputer.transform(values)\n",
    "    X_train.iloc[:,2:] = imputed_train_values\n",
    "    print(f'Missing train values: {np.isnan(imputed_train_values).sum()}')\n",
    "    missing_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "bankruptcy_comp_df_reset_index = X_train.reset_index()\n",
    "bankruptcy_comp_df_reset_index.to_feather(path+'0after_imputation_train.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.6. Impute test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_computed_set:\n",
    "    X_test = pd.read_feather(path+'0after_imputation_test.feather')\n",
    "    X_test = X_test.set_index('index')\n",
    "elif missing_values_count > 0:\n",
    "    X_test = X_test.replace('NaN', np.nan)\n",
    "    values = X_test.iloc[:,2:].values\n",
    "    imputed_test_values = imputer.transform(values)\n",
    "    X_test.iloc[:,2:] = imputed_test_values\n",
    "    print(f'Missing test values: {np.isnan(imputed_test_values).sum()}')\n",
    "    missing_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "if allow_computed_set == False:\n",
    "    bankruptcy_comp_df_reset_index = X_test.reset_index()\n",
    "    bankruptcy_comp_df_reset_index.to_feather(path+'0after_imputation_test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_subsample:\n",
    "    X_sub, _, y_sub, _ = train_test_split(X_train, y_train, test_size=subsample_size_inversed, random_state=1, stratify=y_train)\n",
    "    \n",
    "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_sub, y_sub, test_size=tt_size, random_state=1)\n",
    "    print(X_train_sub.shape, X_test_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [X_train, X_test, X_train_sub, X_test_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Create ratios and growths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1. Create macroeconomic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ending = ['last Year', 'Year - 1', 'Year - 2', 'Year - 3', 'Year - 4', 'Year - 5']\n",
    "new_variables_ending = ['t', 't-1', 't-2', 't-3', 't-4']\n",
    "#Special case for government deficit\n",
    "\n",
    "for frame in df_list:\n",
    "    for i in range(len(variables_ending)):\n",
    "        frame['Deficit GDP ' + variables_ending[i]] = frame['Revenue GDP '+variables_ending[i]] - frame['Expense GDP '+variables_ending[i]]\n",
    "        frame.drop(columns=['Revenue GDP '+variables_ending[i],'Expense GDP '+variables_ending[i]], axis=1, inplace=True)\n",
    "\n",
    "    macro_variables_name = ['Inflation', 'Deficit GDP', 'Unemployment Rate', 'Reserves', 'Interest Rates', 'CPI', 'GNI']\n",
    "    macro_variables_growth_name = ['Inflation growth', 'Deficit growth', 'Unemployment growth', 'Reserves growth', 'Interest rates growth', 'CPI growth', 'GNI growth']\n",
    "\n",
    "    for i in range(len(macro_variables_name)):\n",
    "        for k in range(len(variables_ending)-1):\n",
    "            macro_present = macro_variables_name[i] + \" \" + variables_ending[k]\n",
    "            macro_past = macro_variables_name[i] + \" \" + variables_ending[k+1]\n",
    "            name = macro_variables_growth_name[i] + \" \" + new_variables_ending[k]\n",
    "            \n",
    "            frame[name] = (frame[macro_present]-frame[macro_past])/frame[macro_past]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2. Remove macroeconomic non-growth variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in df_list:\n",
    "    for i in range(len(macro_variables_name)):\n",
    "        for k in range(len(variables_ending)):\n",
    "            macro_present = macro_variables_name[i] + \" \" + variables_ending[k]\n",
    "            \n",
    "            frame.drop(columns=[macro_present], axis=1, inplace=True)\n",
    "            \n",
    "    frame.drop(columns=['GDP growth Year - 5'], axis=1, inplace=True)\n",
    "    frame.rename(columns={'GDP growth last Year':'GDP growth t', 'GDP growth Year - 1':'GDP growth t-1', 'GDP growth Year - 2':'GDP growth t-2', \n",
    "                            'GDP growth Year - 3':'GDP growth t-3', 'GDP growth Year - 4':'GDP growth t-4'}, inplace=True)\n",
    "\n",
    "    print(f'Removed {12+i*k+1} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.3. Create financial ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ending = variables_ending[:-1]\n",
    "financial_variables_dividend = ['Net income', 'Current assets', 'Sales', 'Current assets', 'Cash flow', 'Net income', 'Cash & cash equivalent', 'Net income']\n",
    "financial_variables_divider = ['Total assets', 'Current liabilities', 'Total assets', 'Total assets', 'Total assets', 'Financial expenses',\n",
    "                               'Current liabilities', 'Sales']\n",
    "financial_variables_quotient = ['PR1', 'LiR1', 'OR1', 'LiR2', 'CFR1', 'SR1', 'LiR3', 'PR2']\n",
    "\n",
    "for frame in df_list:\n",
    "    for i in range(len(financial_variables_divider)):\n",
    "        for k in range(len(variables_ending)):\n",
    "            dividend = financial_variables_dividend[i]+\" \"+variables_ending[k]\n",
    "            divider = financial_variables_divider[i]+\" \"+variables_ending[k]\n",
    "            quotient = financial_variables_quotient[i]+\" \"+new_variables_ending[k]\n",
    "            frame[quotient] = frame[dividend]/frame[divider]\n",
    "\n",
    "    #Special case for CSR1\n",
    "    financial_variables_dividend1 = 'Current liabilities'\n",
    "    financial_variables_dividend2 = 'Non-current liabilities'\n",
    "    financial_variables_divider_CSR1 = 'Total assets'\n",
    "    for k in range(len(variables_ending)):\n",
    "        dividend1 = financial_variables_dividend1+\" \"+variables_ending[k]\n",
    "        dividend2 = financial_variables_dividend2+\" \"+variables_ending[k]\n",
    "        divider = financial_variables_divider_CSR1+\" \"+variables_ending[k]\n",
    "        quotient = \"CSR1 \"+new_variables_ending[k]\n",
    "        \n",
    "        frame[quotient] = (frame[dividend1]+frame[dividend2])/frame[divider]\n",
    "\n",
    "    #Special case for CFR2\n",
    "    financial_variables_dividend_CFR2 = 'Cash flow'\n",
    "    financial_variables_divider1 = 'Current liabilities'\n",
    "    financial_variables_divider2 = 'Non-current liabilities'\n",
    "    for k in range(len(variables_ending)):\n",
    "        dividend = financial_variables_dividend_CFR2+\" \"+variables_ending[k]\n",
    "        divider1 = financial_variables_divider1+\" \"+variables_ending[k]\n",
    "        divider2 = financial_variables_divider2+\" \"+variables_ending[k]\n",
    "        quotient = \"CFR2 \"+new_variables_ending[k]\n",
    "            \n",
    "        frame[quotient] = frame[dividend]/(frame[divider1]+frame[divider2])\n",
    "\n",
    "    #Special case for GR1\n",
    "    financial_variables_combined = 'Total assets'\n",
    "    for k in range(len(variables_ending)-1):\n",
    "        dividend1 = financial_variables_combined+\" \"+variables_ending[k]\n",
    "        dividend2 = financial_variables_combined+\" \"+variables_ending[k+1]\n",
    "        divider = financial_variables_combined+\" \"+variables_ending[k+1]\n",
    "        quotient = \"GR1 \"+new_variables_ending[k]\n",
    "        \n",
    "        frame[quotient] = (frame[dividend1]-frame[dividend2])/frame[divider]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.4. Remove financial non-ratio variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_variables_to_remove = ['Net income', 'Current assets', 'Sales', 'Total assets', 'Cash flow', 'Cash & cash equivalent', 'Current liabilities', \n",
    "                                 'Non-current liabilities', 'Financial expenses']\n",
    "for frame in df_list:\n",
    "    for i in range(len(financial_variables_to_remove)):\n",
    "        for k in range(len(variables_ending)):\n",
    "            frame.drop(columns=[financial_variables_to_remove[i] + \" \" + variables_ending[k]], axis=1, inplace=True)\n",
    "            \n",
    "    print(f'{i*k} columns deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.5. Remove rows with inf or -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = X_train.shape[0]\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t'] == np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t'] == np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t'] == -np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t'] == -np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "y_train = y_train.drop(X_train[X_train['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "X_train = X_train.drop(X_train[X_train['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "new_shape = X_train.shape[0]\n",
    "print(f'removed {old_shape - new_shape} new shapes {X_train.shape} & {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = X_test.shape[0]\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t'] == np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t'] == np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t'] == -np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t'] == -np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "y_test = y_test.drop(X_test[X_test['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "X_test = X_test.drop(X_test[X_test['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "new_shape = X_test.shape[0]\n",
    "print(f'removed {old_shape - new_shape} new shapes {X_test.shape} & {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = X_train_sub.shape[0]\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t'] == np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t'] == np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t'] == -np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t'] == -np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "y_train_sub = y_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "X_train_sub = X_train_sub.drop(X_train_sub[X_train_sub['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "new_shape = X_train_sub.shape[0]\n",
    "print(f'removed {old_shape - new_shape} new shapes {X_train_sub.shape} & {y_train_sub.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_shape = X_test_sub.shape[0]\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t'] == np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t'] == np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t'] == -np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t'] == -np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-1'] == np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-1'] == -np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-2'] == np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-2'] == -np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-3'] == np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-3'] == -np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-4'] == np.inf].index, axis=0)\n",
    "y_test_sub = y_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "X_test_sub = X_test_sub.drop(X_test_sub[X_test_sub['CFR2 t-4'] == -np.inf].index, axis=0)\n",
    "new_shape = X_test_sub.shape[0]\n",
    "print(f'removed {old_shape - new_shape} new shapes {X_test_sub.shape} & {y_test_sub.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Downcast datatypes\n",
    "to reduce memory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in df_list:\n",
    "    frame.info()\n",
    "    print('--'*60)\n",
    "    frame.iloc[:,2:] = frame.iloc[:,2:].apply(pd.to_numeric, downcast='float')\n",
    "    print(frame.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Check highly correlated features\n",
    "Results in the same features being removed then removing then doing it sequentially, thus the computationally more efficient approach is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = []\n",
    "correlated_features_names = []\n",
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9 and X_train.columns.get_loc(correlation_matrix.columns[i]) not in correlated_features:\n",
    "            correlated_features.append(X_train.columns.get_loc(correlation_matrix.columns[i]))\n",
    "            correlated_features_names.append(correlation_matrix.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlated_features_names)\n",
    "print(len(correlated_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Display more detailed infos of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{bankruptcy_comp_df[\"Bankrupt\"].value_counts()[0]} non-bankrupt companies - {round(bankruptcy_comp_df[\"Bankrupt\"].value_counts()[0]/len(bankruptcy_comp_df)*100,2)}% of the dataset')\n",
    "print(f'{bankruptcy_comp_df[\"Bankrupt\"].value_counts()[1]} bankrupt companies - {round(bankruptcy_comp_df[\"Bankrupt\"].value_counts()[1]/len(bankruptcy_comp_df)*100,2)}% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.1. Show class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=bankruptcy_comp_df['Bankrupt'], palette=['#000000','#808080'])\n",
    "plt.title('Class distribution: non-bankrupt and bankrupt firms', fontsize=14, pad=20)\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.yticks(np.arange(0, 2500000, 500000))\n",
    "plt.ylabel('Number of firms')\n",
    "plt.text(x=-0.19, y=bankruptcy_comp_df[\"Bankrupt\"].value_counts()[0]+10000, s=f'n={bankruptcy_comp_df[\"Bankrupt\"].value_counts()[0]:,}')\n",
    "plt.text(x=0.83, y=bankruptcy_comp_df[\"Bankrupt\"].value_counts()[1]+20000, s=f'n={bankruptcy_comp_df[\"Bankrupt\"].value_counts()[1]:,}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.2. Show distribution for all features of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.hist(figsize=(50,50), bins=50, edgecolor='black', color='Grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "ax = sns.boxplot(data=X_train, orient='h', palette='Greys')\n",
    "ax.set_title('Variables Boxplots', fontsize=40)\n",
    "ax.set(xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_train.corr()\n",
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "sns.heatmap(corr, ax=ax, cmap='Greys', linewidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.3. Show distribution for all features of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.hist(figsize=(50,50), bins=50, edgecolor='black', color='Grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "ax = sns.boxplot(data=X_test, orient='h', palette='Greys')\n",
    "ax.set_title('Variables Boxplots', fontsize=40)\n",
    "ax.set(xscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_test.corr()\n",
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "sns.heatmap(corr, ax=ax, cmap='Greys', linewidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.4. Show distribution of complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_df = pd.concat([X_train, X_test])\n",
    "print(bankruptcy_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('complete_list.npy', bankruptcy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all variables, their mean, and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_list = []\n",
    "\n",
    "for col in bankruptcy_df.columns:\n",
    "    if col == 'Country ISO code' or col == 'Last avail. Year':\n",
    "        continue\n",
    "    name = col\n",
    "    mean = bankruptcy_df[col].mean()\n",
    "    std = bankruptcy_df[col].std()\n",
    "    stats_list.append([name, mean, std])\n",
    "\n",
    "print(len(stats_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stats in stats_list:\n",
    "    print(f'{stats[0]} & {round(stats[1],4)} & {round(stats[2],4)} & \\\\\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.5. Selected distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_growth_t = bankruptcy_comp_df['GDP growth t'].loc[bankruptcy_comp_df['Bankrupt'] == 1].value\n",
    "sns.displot(GDP_growth_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8.6. Different distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bankruptcy_comp_df.groupby('Country ISO code').Bankrupt.agg(no_bankrupt=('sum'), no_firms=('count'), bankrupt_percentage=(lambda x: str(round(x.sum()/x.count()*100,4))+\"%\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bankruptcy_comp_df.groupby('Last avail. Year').Bankrupt.agg(no_bankrupt=('sum'), no_non_bankrupt=(lambda x: str(x.count()-x.sum())), no_firms=('count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating datasets (Models 1-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Bankrupt'] = y_train\n",
    "X_test['Bankrupt'] = y_test\n",
    "X_train_sub['Bankrupt'] = y_train_sub\n",
    "X_test_sub['Bankrupt'] = y_test_sub\n",
    "X_train_for_smoteenn = X_train\n",
    "X_test_for_smoteenn = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Info on training & testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)\n",
    "X_test = X_test.drop(columns=['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)\n",
    "X_train_sub = X_train_sub.drop(columns=['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)\n",
    "X_test_sub = X_test_sub.drop(columns=['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Creating models 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Complete (Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X train: {X_train.shape} | Shape of y train: {y_train.shape} | Percentage bankrupt: {round(y_train.sum()/y_train.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_test.shape} | Shape of y test: {y_test.shape} | Percentage bankrupt: {round(y_test.sum()/y_test.count()*100,3)}%')\n",
    "if allow_subsample:\n",
    "    print(f'Shape of X_sub: {X_train_sub.shape} | Shape of y_sub: {y_train_sub.shape} | Percentage bankrupt: {round(y_train_sub.sum()/y_train_sub.count()*100,3)}%')\n",
    "    print(f'Shape of X_sub: {X_test_sub.shape} | Shape of y_sub: {y_test_sub.shape} | Percentage bankrupt: {round(y_test_sub.sum()/y_test_sub.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "if allow_subsample:\n",
    "    X_train_sub = scaler.fit_transform(X_train_sub)\n",
    "    X_test_sub = scaler.fit_transform(X_test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Without time (Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_train = X_train[:,::5]\n",
    "X_y_test = X_test[:,::5]\n",
    "print(f'Shape of X train: {X_y_train.shape} | Shape of y train: {y_train.shape} | Percentage bankrupt: {round(y_train.sum()/y_train.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_y_test.shape} | Shape of y test: {y_test.shape} | Percentage bankrupt: {round(y_test.sum()/y_test.count()*100,3)}%')\n",
    "if allow_subsample:\n",
    "    X_y_train_sub = X_train_sub[:,::5]\n",
    "    X_y_test_sub = X_test_sub[:,::5]\n",
    "    print(f'Shape of X_sub: {X_y_train_sub.shape} | Shape of y_sub: {y_train_sub.shape} | Percentage bankrupt: {round(y_train_sub.sum()/y_train_sub.count()*100,3)}%')\n",
    "    print(f'Shape of X_sub: {X_y_test_sub.shape} | Shape of y_sub: {y_test_sub.shape} | Percentage bankrupt: {round(y_test_sub.sum()/y_test_sub.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. Without macro (Model 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m_train = X_train[:,40:]\n",
    "X_m_test = X_test[:,40:]\n",
    "print(f'Shape of X train: {X_m_train.shape} | Shape of y train: {y_train.shape} | Percentage bankrupt: {round(y_train.sum()/y_train.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_m_test.shape} | Shape of y test: {y_test.shape} | Percentage bankrupt: {round(y_test.sum()/y_test.count()*100,3)}%')\n",
    "if allow_subsample:\n",
    "    X_m_train_sub = X_train_sub[:,40:]\n",
    "    X_m_test_sub = X_test_sub[:,40:]\n",
    "    print(f'Shape of X_sub: {X_m_train_sub.shape} | Shape of y_sub: {y_train_sub.shape} | Percentage bankrupt: {round(y_train_sub.sum()/y_train_sub.count()*100,3)}%')\n",
    "    print(f'Shape of X_sub: {X_m_test_sub.shape} | Shape of y_sub: {y_test_sub.shape} | Percentage bankrupt: {round(y_test_sub.sum()/y_test_sub.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Without time and macro (Model 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_m_train = X_m_train[:,::5]\n",
    "X_y_m_test = X_m_test[:,::5]\n",
    "print(f'Shape of X train: {X_y_m_train.shape} | Shape of y train: {y_train.shape} | Percentage bankrupt: {round(y_train.sum()/y_train.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_y_m_test.shape} | Shape of y test: {y_test.shape} | Percentage bankrupt: {round(y_test.sum()/y_test.count()*100,3)}%')\n",
    "if allow_subsample:\n",
    "    X_y_m_train_sub = X_m_train_sub[:,::5]\n",
    "    X_y_m_test_sub = X_m_test_sub[:,::5]\n",
    "    print(f'Shape of X_sub: {X_y_m_train_sub.shape} | Shape of y_sub: {y_train_sub.shape} | Percentage bankrupt: {round(y_train_sub.sum()/y_train_sub.count()*100,3)}%')\n",
    "    print(f'Shape of X_sub: {X_y_m_test_sub.shape} | Shape of y_sub: {y_test_sub.shape} | Percentage bankrupt: {round(y_test_sub.sum()/y_test_sub.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get index of highly correlated features to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_for_smoteenn_remove = X_train_for_smoteenn.drop(columns=['Country ISO code', 'Last avail. Year', 'Bankrupt'], axis=1)\n",
    "remove_complete = [False]*X_train_for_smoteenn_remove.shape[1]\n",
    "for el in correlated_features:\n",
    "    remove_complete[(int(el)-2)] = True\n",
    "    \n",
    "remove_complete_y = remove_complete[::5]\n",
    "remove_complete_m = remove_complete[40:]\n",
    "remove_complete_y_m = remove_complete_m[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove highly correlated features from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_complete = [i for i, x in enumerate(remove_complete) if x]\n",
    "remove_complete_y = [i for i, x in enumerate(remove_complete_y) if x]\n",
    "remove_complete_m = [i for i, x in enumerate(remove_complete_m) if x]\n",
    "remove_complete_y_m = [i for i, x in enumerate(remove_complete_y_m) if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, remove_complete, axis=1)\n",
    "X_y_train = np.delete(X_y_train, remove_complete_y, axis=1)\n",
    "X_m_train = np.delete(X_m_train, remove_complete_m, axis=1)\n",
    "X_y_m_train = np.delete(X_y_m_train, remove_complete_y_m, axis=1)\n",
    "X_train_sub = np.delete(X_train_sub, remove_complete, axis=1)\n",
    "X_y_train_sub = np.delete(X_y_train_sub, remove_complete_y, axis=1)\n",
    "X_m_train_sub = np.delete(X_m_train_sub, remove_complete_m, axis=1)\n",
    "X_y_m_train_sub = np.delete(X_y_m_train_sub, remove_complete_y_m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.delete(X_test, remove_complete, axis=1)\n",
    "X_y_test = np.delete(X_y_test, remove_complete_y, axis=1)\n",
    "X_m_test = np.delete(X_m_test, remove_complete_m, axis=1)\n",
    "X_y_m_test = np.delete(X_y_m_test, remove_complete_y_m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X train number of features: {len(X_train[0])}')\n",
    "print(f'X y train number of features: {len(X_y_train[0])}')\n",
    "print(f'X m train number of features: {len(X_m_train[0])}')\n",
    "print(f'X y m train number of features: {len(X_y_m_train[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['complete', 'without years', 'without macro', 'without years and macro', 'complete SMOTEENN', 'without years SMOTEENN', 'without macro SMOTEENN', 'without years and macro SMOTEENN']\n",
    "\n",
    "if allow_subsample_hyperparameter:\n",
    "    X_list_m = [[X_train_sub, X_test_sub], [X_y_train_sub, X_y_test_sub], [X_m_train_sub, X_m_test_sub], [X_y_m_train_sub, X_y_m_test_sub], ]\n",
    "    y_list_m = [(y_train_sub, y_test_sub), (y_train_sub, y_test_sub), (y_train_sub, y_test_sub), (y_train_sub, y_test_sub)]\n",
    "else:\n",
    "    X_list_m = [[X_train, X_test], [X_y_train, X_y_test], [X_m_train, X_m_test], [X_y_m_train, X_y_m_test]]\n",
    "    y_list_m = [(y_train, y_test), (y_train, y_test), (y_train, y_test), (y_train, y_test)]\n",
    "\n",
    "X_list = [[X_train, X_test], [X_y_train, X_y_test], [X_m_train, X_m_test], [X_y_m_train, X_y_m_test]]\n",
    "y_list = [(y_train, y_test), (y_train, y_test), (y_train, y_test), (y_train, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if allow_computed_set == False:\n",
    "    bankruptcy_comp_df_reset_index = bankruptcy_comp_df.reset_index()\n",
    "    bankruptcy_comp_df_reset_index.to_feather(path+'1ready_for_smoteenn.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sme = SMOTEENN(smote=SMOTE(sampling_strategy='minority', random_state=1), enn=EditedNearestNeighbours(sampling_strategy='all'), random_state=1, n_jobs=jobs)\n",
    "    \n",
    "print(X_train_for_smoteenn.groupby('Country ISO code').Bankrupt.agg(no_bankrupt=('sum'), no_firms=('count'), bankrupt_percentage=(lambda x: str(round(x.sum()/x.count()*100,4))+\"%\")))\n",
    "print('--'*60)\n",
    "\n",
    "if allow_computed_set:\n",
    "    X_train_smoteenn = pd.read_feather(path+'2after_smoteenn.feather')\n",
    "    X_train_smoteenn = X_train_smoteenn.set_index('index')\n",
    "    print(X_train_smoteenn.shape)\n",
    "else:\n",
    "    countries = X_train_for_smoteenn['Country ISO code'].unique()\n",
    "    print(countries)\n",
    "    print('--'*60)\n",
    "    \n",
    "    all_countries = []\n",
    "    \n",
    "    for country in countries:\n",
    "        \n",
    "        if country == 'IT':\n",
    "            it_df = X_train_for_smoteenn.loc[X_train_for_smoteenn['Country ISO code']=='IT']\n",
    "\n",
    "            print(it_df.shape)\n",
    "\n",
    "            X_it = it_df.drop(['Bankrupt', 'Country ISO code'], axis=1)\n",
    "            y_it = it_df['Bankrupt']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_it, y_it, test_size=0.3333, random_state=1, stratify=it_df['Bankrupt'])\n",
    "\n",
    "            X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train, test_size=0.5, random_state=1, stratify=y_train)\n",
    "            X_test2 = X_test\n",
    "            y_test2 = y_test\n",
    "\n",
    "            X_train1['Bankrupt'] = y_train1\n",
    "            X_test1['Bankrupt'] = y_test1\n",
    "            X_test2['Bankrupt'] = y_test2\n",
    "            print(X_train1.shape, X_test1.shape, X_test2.shape)\n",
    "\n",
    "            datasets = [X_train1, X_test1, X_test2]\n",
    "\n",
    "            all_it = []\n",
    "            i = 0\n",
    "            for dataset in datasets:\n",
    "                i += 1\n",
    "                print(f'{i} started')\n",
    "                X_temp = dataset.drop(['Bankrupt'], axis=1)\n",
    "                y_temp = dataset['Bankrupt']\n",
    "                print(X_temp.shape, y_temp.shape)\n",
    "                X_temp_resampled, y_temp_resampled = sme.fit_resample(X_temp, y_temp)\n",
    "                temp_comp_df = pd.concat([y_temp_resampled, X_temp_resampled], axis=1)\n",
    "                temp_comp_df['Country ISO code']='IT'\n",
    "                print('IT '+str(i) + ' of 3 done!')\n",
    "                all_countries.append(temp_comp_df)\n",
    "        else:\n",
    "            print(f'{country} started')\n",
    "            temp_df = X_train_for_smoteenn.loc[X_train_for_smoteenn['Country ISO code']==country]\n",
    "            X_temp = temp_df.drop(['Bankrupt', 'Country ISO code'], axis=1)\n",
    "            y_temp = temp_df['Bankrupt']\n",
    "            X_temp_resampled, y_temp_resampled = sme.fit_resample(X_temp, y_temp)\n",
    "            temp_comp_df = pd.concat([y_temp_resampled, X_temp_resampled], axis=1)\n",
    "            temp_comp_df['Country ISO code']=country\n",
    "            all_countries.append(temp_comp_df)\n",
    "            print(f'{country} done!')\n",
    "    \n",
    "    print('--'*60)\n",
    "    X_train_smoteenn = pd.concat(all_countries)\n",
    "\n",
    "print(X_train_smoteenn.groupby('Country ISO code').Bankrupt.agg(no_bankrupt=('sum'), no_firms=('count'), bankrupt_percentage=(lambda x: str(round(x.sum()/x.count()*100,4))+\"%\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = [X_train_smoteenn, X_test_for_smoteenn]\n",
    "bankruptcy_smoteenn_df = pd.concat(to_concat)\n",
    "if allow_computed_set == False:\n",
    "    bankruptcy_comp_df_reset_index = X_train_smoteenn.reset_index()\n",
    "    bankruptcy_comp_df_reset_index.to_feather(path+'2after_smoteenn.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Splitting into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = X_train_smoteenn.drop(['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)\n",
    "y_train_resampled = X_train_smoteenn['Bankrupt']\n",
    "X_test_resampled = X_test_for_smoteenn.drop(['Bankrupt', 'Country ISO code', 'Last avail. Year'], axis=1)\n",
    "y_test_resampled = X_test_for_smoteenn['Bankrupt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Creating datasets (Models 5-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Complete SMOTEENN (Model 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of X train: {X_train_resampled.shape} | Shape of y train: {y_train_resampled.shape} | Percentage bankrupt training: {round(y_train_resampled.sum()/y_train_resampled.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_test_resampled.shape} | Shape of y test: {y_test_resampled.shape} | Percentage bankrupt testing: {round(y_test_resampled.sum()/y_test_resampled.count()*100,3)}%')\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_resampled = scaler.fit_transform(X_test_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Without time (Model 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_train_resampled = X_train_resampled[:,::5]\n",
    "X_y_test_resampled = X_test_resampled[:,::5]\n",
    "print(f'Shape of X train: {X_y_train_resampled.shape} | Shape of y train: {y_train_resampled.shape} | Percentage bankrupt: {round(y_train_resampled.sum()/y_train_resampled.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_y_test_resampled.shape} | Shape of y test: {y_test_resampled.shape} | Percentage bankrupt: {round(y_test_resampled.sum()/y_test_resampled.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Without macro (Model 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m_train_resampled = X_train_resampled[:,40:]\n",
    "X_m_test_resampled = X_test_resampled[:,40:]\n",
    "print(f'Shape of X train: {X_m_train_resampled.shape} | Shape of y train: {y_train_resampled.shape} | Percentage bankrupt: {round(y_train_resampled.sum()/y_train_resampled.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_m_test_resampled.shape} | Shape of y test: {y_test_resampled.shape} | Percentage bankrupt: {round(y_test_resampled.sum()/y_test_resampled.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4. Without time and macro (Model 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_m_train_resampled = X_m_train_resampled[:,::5]\n",
    "X_y_m_test_resampled = X_m_test_resampled[:,::5]\n",
    "print(f'Shape of X train: {X_y_m_train_resampled.shape} | Shape of y train: {y_train_resampled.shape} | Percentage bankrupt: {round(y_train_resampled.sum()/y_train_resampled.count()*100,3)}%')\n",
    "print(f'Shape of X test: {X_y_m_test_resampled.shape} | Shape of y test: {y_test_resampled.shape} | Percentage bankrupt: {round(y_test_resampled.sum()/y_test_resampled.count()*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Remove features with high correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = np.delete(X_train_resampled, remove_complete, axis=1)\n",
    "X_y_train_resampled = np.delete(X_y_train_resampled, remove_complete_y, axis=1)\n",
    "X_m_train_resampled = np.delete(X_m_train_resampled, remove_complete_m, axis=1)\n",
    "X_y_m_train_resampled = np.delete(X_y_m_train_resampled, remove_complete_y_m, axis=1)\n",
    "\n",
    "print(f'X train resampled number of features: {len(X_train_resampled[0])}')\n",
    "print(f'X y train resampled number of features: {len(X_y_train_resampled[0])}')\n",
    "print(f'X m train resampled number of features: {len(X_m_train_resampled[0])}')\n",
    "print(f'X y m train resampled number of features: {len(X_y_m_train_resampled[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_resampled = np.delete(X_test_resampled, remove_complete, axis=1)\n",
    "X_y_test_resampled = np.delete(X_y_test_resampled, remove_complete_y, axis=1)\n",
    "X_m_test_resampled = np.delete(X_m_test_resampled, remove_complete_m, axis=1)\n",
    "X_y_m_test_resampled = np.delete(X_y_m_test_resampled, remove_complete_y_m, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Append to X_list & y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list.append([X_train_resampled, X_test_resampled])\n",
    "X_list.append([X_y_train_resampled, X_y_test_resampled])\n",
    "X_list.append([X_m_train_resampled, X_m_test_resampled])\n",
    "X_list.append([X_y_m_train_resampled, X_y_m_test_resampled])\n",
    "y_list.append((y_train_resampled, y_test_resampled))\n",
    "y_list.append((y_train_resampled, y_test_resampled))\n",
    "y_list.append((y_train_resampled, y_test_resampled))\n",
    "y_list.append((y_train_resampled, y_test_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Display class distribution after SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=bankruptcy_smoteenn_df['Bankrupt'], palette=['#000000','#808080'])\n",
    "plt.title('Class distribution: non-bankrupt and bankrupt firms', fontsize=14, pad=20)\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "plt.yticks(np.arange(0, 2500000, 500000))\n",
    "plt.ylabel('Number of firms')\n",
    "plt.text(x=-0.19, y=bankruptcy_smoteenn_df[\"Bankrupt\"].value_counts()[0]+10000, s=f'n={bankruptcy_smoteenn_df[\"Bankrupt\"].value_counts()[0]:,}')\n",
    "plt.text(x=0.83, y=bankruptcy_smoteenn_df[\"Bankrupt\"].value_counts()[1]+20000, s=f'n={bankruptcy_smoteenn_df[\"Bankrupt\"].value_counts()[1]:,}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Display combined class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16,8), sharey=True)\n",
    "fig.tight_layout(pad=6.0)\n",
    "sns.countplot(x=bankruptcy_comp_df[\"Bankrupt\"], palette=['#000000','#808080'], ax=ax[0])\n",
    "sns.countplot(x=bankruptcy_smoteenn_df['Bankrupt'], palette=['#000000','#808080'], ax=ax[1])\n",
    "ax[0].set_title('Class distribution: non-bankrupt and bankrupt firms', fontsize=18, pad=20)\n",
    "ax[0].set_ylabel('Number of firms', fontsize=12)\n",
    "ax[0].set_yticks(np.arange(0, 2500000, 500000))\n",
    "ax[0].set_yticklabels(np.arange(0, 2500000, 500000), fontsize=12)\n",
    "ax[1].set_title('Class distribution: non-bankrupt and bankrupt firms\\nwith SMOTEENN', fontsize=18, pad=20)\n",
    "ax[1].set_ylabel('Number of firms', fontsize=12)\n",
    "ax[1].set_yticks(np.arange(0, 2500000, 500000))\n",
    "ax[1].set_yticklabels(np.arange(0, 2500000, 500000), fontsize=12)\n",
    "ax[1].yaxis.set_tick_params(labelbottom=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_list.npy', X_list)\n",
    "np.save('X_list_m.npy', X_list_m)\n",
    "np.save('y_list.npy', y_list)\n",
    "np.save('y_list_m.npy', y_list_m)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aef203514c858290544fd6a14248b5f64855080360fc299c24eb5474b48fd873"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
